<!-- python -m readme2tex --output papers.md --nocdn --rerender papers_raw.md -->
## –ö–æ–Ω—Å–ø–µ–∫—Ç—ã —Å—Ç–∞—Ç–µ–π 
- **Accurate and Robust Feature Importance Estimation under Distribution Shifts, 2020**  [[paper]](https://arxiv.org/pdf/2009.14454.pdf)
  - –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π: –æ—Å–Ω–æ–≤–Ω–∞—è —Å–µ—Ç—å –æ–±—É—á–∞–µ—Ç—Å—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π (second net), —É –∫–æ—Ç–æ—Ä–æ–π:
    - —Ü–µ–ª—å - –Ω–∞—É—á–∏—Ç—å—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å loss –æ—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ç–∏
    - input - –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª–æ—ë–≤ –æ—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ç–∏
    - loss
      - contrastive training - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–Ω–∏–µ –ø–∞—Ä —Å–∫–æ—Ä–æ–≤
      - dropout calibration - hinge loss + –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
  - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Granger –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏—á–∏–Ω–Ω–æ—Å—Ç–∏ (—Å–≤—è–∑—å –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–æ–º –∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π —Å—É—â–µ—Ç—Å–≤—É–µ—Ç, –µ—Å–ª–∏ –∫–∞—á–µ—Å—Ç–≤–æ —Ç–æ–ª—å–∫–æ —É—Ö—É–¥—à–∏—Ç—Å—è –ø—Ä–∏ –æ—Ç–±—Ä–∞—Å—ã–≤–∞–Ω–∏–∏ –¥–∞–Ω–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞)
  - –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞ - —Ä–∞–∑–Ω–∏—Ü–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–æ–π —Å–µ—Ç–∏ —Å –µ–≥–æ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –±–µ–∑  
  *–ò—Ç–æ–≥*:
    - –Ω–∞ 15-30 % –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ, —á–µ–º —É Shap
    - –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ —Ä–∞–∑–ª–∏—á–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è x_test, –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å x_train, loss second net –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ —Ä–∞—Å—Ç—ë—Ç
    - –ø–æ–¥—Ö–æ–¥ —É—Å—Ç–æ–π—á–∏–≤–µ–π –ø—Ä–∏ —Å–∏–ª—å–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö x_test Deep_Shap'–∞ –≤ 2 —Ä–∞–∑–∞

- **Feature Importance Ranking for Deep Learning, 2020** [[paper]](https://arxiv.org/pdf/2010.08973.pdf)
  - —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –¥–≤–µ —Å–µ—Ç–∏ operator net –∏ selector net, –º–∞—Å–∫–∏ –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ - –±–∏–Ω–∞—Ä–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–∞ (1 - –±–µ—Ä–µ–º –ø—Ä–∏–∑–Ω–∞–∫, 0 - –Ω–µ—Ç), –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª-–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ - –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä
  - –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø–æ–æ—á–µ—Ä–µ–¥–Ω–æ
  - operator net:
    - —Ü–µ–ª—å - –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏
    - input - x –∏ –º–∞—Å–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    - loss - —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∑–∞–¥–∞—á–µ
  - selector net:
    - —Ü–µ–ª—å - –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å loss operator net
    - input - –º–∞—Å–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    - loss - l2 —Å loss'–æ–º, –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–º –æ—Ç operator net
  - –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞ - —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ loss'–∞ selector net'–∞ –≤ —Ç–æ—á–∫–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
  - –ø—Ä–æ—Ü–µ—Å—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –æ—á–µ–Ω—å –¥–æ–ª–≥–∏–π  
  *–ò—Ç–æ–≥*:
    - –≤ —Å—Ä–µ–¥–Ω–µ–º –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    - –ª—É—á—à–µ–µ RFE, BAHSIC, mRMR, CCM –Ω–∞ 4-—ë—Ö benchmark –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö

- **Knockoffs for the mass: new feature importance statistics with false discovery guarantees, 2019** [[paper]](https://arxiv.org/pdf/1807.06214.pdf)
  - –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä—É–µ—Ç—Å—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (—Ç–æ–ª—å–∫–æ –ø—Ä–∏–∑–Ω–∞–∫–∏) –±–∞–π–µ—Å–æ–≤—Å–∫–∏–º–∏ —Å–µ—Ç—è–º–∏
  - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –≤—ã–±–æ—Ä–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–º –æ–±—Ä–∞–∑–æ–º (—á—Ç–æ–±—ã –Ω–µ –≤—ã—Ö–æ–¥–∏—Ç—å –∑–∞ –∏—Å—Ö–æ–¥–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
  - –≤–º–µ—Å—Ç–æ —Ç–æ–≥–æ, —á—Ç–æ–±—ã –ø–µ—Ä–µ–º–µ—à–∏–≤–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–∞ (–≤ permutation importance), –±–µ—Ä–µ—Ç—Å—è –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ –∏–∑ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏
  - –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞ - –ø–ª–æ—â–∞–¥—å –ø–æ–¥ –∫—Ä–∏–≤–æ–π (y - –¥–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, x - –ø–∞—Ä–∞–º–µ—Ç—Ä –≤–∑–≤–µ—à–µ–Ω–Ω–æ–π —Å—É–º–º—ã) –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, [0, 10])
  - FDR –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω–µ–ª—å–∑—è –æ—Ü–µ–Ω–∏—Ç—å, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ –º—ã —Ö–æ—Ä–æ—à–æ –º–æ–¥–µ–ª–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∫–∏

- **A Unified Approach to Interpreting Model Predictions, 2017** [[paper]](https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf)
  - —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è —Å–µ–º–µ–π—Å—Ç–≤–æ –∞–¥–¥–∏—Ç–∏–≤–Ω—ã—Ö explanation models
  - –≤ –¥–∞–Ω–Ω–æ–º –∫–ª–∞—Å—Å–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è explanation model, —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—é—â–∞—è —Å–≤–æ–π—Å—Ç–≤–∞–º:
    - local accuracy - —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π f(x) –∏ exp_model(x')
    - missingness - –ø—Ä–∏–∑–Ω–∞–∫, –Ω–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏–π –≤ x, –±—É–¥–µ—Ç –∏–º–µ—Ç—å –Ω—É–ª–µ–≤—É—é –≤–∞–∂–Ω–æ—Å—Ç—å
    - consistency - –ø—Ä–∏–∑–Ω–∞–∫ –≤–æ –≤—Å–µ–≤–æ–∑–º–æ–∂–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏—è—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∏–º–µ–µ—Ç –Ω–µ –º–µ–Ω—å—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–∞ f, —á–µ–º –Ω–∞ f' -> –µ–≥–æ –≤–∞–∂–Ω–æ—Å—Ç—å –¥–ª—è f >= –≤–∞–∂–Ω–æ—Å—Ç—å –¥–ª—è f'
  - —Å—á–∏—Ç–∞—Ç—å —Ç–∞–∫—É—é explanation model –¥–æ—Ä–æ–≥–æ
  - Linear LIME + Kernel SHAP –¥–∞—é—Ç –∏—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è SHAP values
  - –≤ —Å–ª—É—á–∞–µ f = max, –º–æ–∂–Ω–æ –∑–∞ –∫–≤–∞–¥—Ä–∞—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ SHAP –ø–æ—Å—á–∏—Ç–∞—Ç—å (–ø–æ–ª—å–∑—É–µ–º—Å—è —Å–≤–æ–π—Å—Ç–≤–∞–º–∏ max)
  - Deep SHAP - –≤–º–µ—Å—Ç–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –≤ DeepLIFT –ø–æ–¥—Å—Ç–∞–≤–ª—è–µ–º SHAP –≤–∞–∂–Ω–æ—Å—Ç—å –¥–ª—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–∞—Å—á—ë—Ç–∞—Ö  
  *–ò—Ç–æ–≥*:
    - –æ–±–æ–±—â–∏–ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–µ—Ç–æ–¥—ã
    - –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–π –∑–∞–¥–∞—á–µ SHAP –≤–∞–∂–Ω–æ—Å—Ç—å —Å–æ–≤–ø–∞–ª–∞ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π
    - –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —É–ª—É—á—à–∏–ª–∏ –≤—Ä–µ–º—è —Ä–∞—Å—á—ë—Ç–∞

- **Learning Important Features Through Propagating Activation Differences, 2017** [[paper]](https://arxiv.org/pdf/1704.02685.pdf)
  - –º–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Ä–∞–∑–Ω–∏—Ü–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ–π—Ä–æ–Ω–æ–≤ –º–µ–∂–¥—É –Ω–∞—á–∞–ª—å–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º (reference) –∏ –∫–æ–Ω–µ—á–Ω—ã–º
  - —Ä–∞–∑–¥–µ–ª—è—é—Ç—Å—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –≤–∫–ª–∞–¥—ã –≤ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
  - –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞ –ª–∏–Ω–µ–π–Ω–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ä–∞–∑–Ω–∏—Ü—ã x - x_reference
  - –≤–∞–∂–Ω–æ—Å—Ç—å - shapley value —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ä–∞–∑–±–∏–µ–Ω–∏–π 2
  - x_reference –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–¥–∞—á–∏  
  *–ò—Ç–æ–≥*:
    - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω–æ—Å—Ç–∏ x - x_reference –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è—Ç—å—Å—è –∫–æ–≥–¥–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç —Ä–∞–≤–µ–Ω –Ω—É–ª—é
    - gradients, gradients*input, guided backprop, rescale rule —Ç–µ—Ä—è—é—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤ —Ö–æ–¥–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö –≤ –æ—Ç–ª–∏—á–∏–∏ –æ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ reveal_cancel rule

- **CXPlain: Causal Explanations for Model Interpretation under Uncertainty, 2019** [[paper]](https://arxiv.org/pdf/1910.12336.pdf)
  - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Granger's definition of causality (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏, –∏—Å—Ö–æ–¥—è —Ç–æ–ª—å–∫–æ –∏–∑ –¥–∞–Ω–Ω—ã—Ö, –Ω–µ–ª—å–∑—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å)
    - –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ
    - –ø—Ä–∏–∑–Ω–∞–∫ –≤—Ä–µ–º–µ–Ω–Ω–æ –ø—Ä–µ–¥—à–µ—Å—Ç–≤—É–µ—Ç –º–µ—Ç–∫–µ (–¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –º–µ—Ç–∫—É, –Ω—É–∂–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–∏–∑–Ω–∞–∫–µ)
  - –∏—Å—Ç–∏–Ω–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞ - –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –æ—à–∏–±–æ–∫ –æ–±—ä—è—Å–Ω—è–µ–º–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ x_mask –∏ x_reference
  - –æ–±—É—á–∞–µ—Ç—Å—è explanation model (–ø–æ–¥—Ö–æ–¥—è—â–∞—è —Ä–µ—à–∞–µ–º–æ–π –∑–∞–¥–∞—á–µ)
    - —Ü–µ–ª—å - –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    - input - –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç x_train
    - loss - —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –ö—É–ª—å–±–∞–∫–∞ ‚Äî –õ–µ–π–±–ª–µ—Ä–∞ –º–µ–∂–¥—É –∏—Å—Ç–∏–Ω–Ω—ã–º –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏ –≤–∞–∂–Ω–æ—Å—Ç–µ–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
  - –¥–ª—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –æ–±—É—á–∞–µ–º –∞–Ω—Å–∞–º–±–ª—å explanation_models (–Ω–∞ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤—ã–±–æ—Ä–∫–∞—Ö), –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å - –º–µ–¥–∏–∞–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∞–Ω—Å–∞–º–±–ª—è, –∞ —Ç–æ—á–Ω–æ—Å—Ç—å - –∏–Ω—Ç–µ—Ä–∫–≤–∞–Ω—Ç–∏–ª—å–Ω—ã–π —Ä–∞–∑–º–∞—Ö  
  *–ò—Ç–æ–≥*:
    - —Ç–æ—á–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç —Å –æ—à–∏–±–∫–æ–π —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    - –ø—Ä–∏ –Ω–µ–±–æ–ª—å—à–æ–π –º–æ—â–Ω–æ—Å—Ç–∏ –∞–Ω—Å–∞–º–±–ª—è (5) —Ö–æ—Ä–æ—à–æ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —Ç–æ—á–Ω–æ—Å—Ç—å explanation_model
    - –∫–∞—á–µ—Å—Ç–≤–æ –ª—É—á—à–µ –Ω–∞ 20%, –±—ã—Å—Ç—Ä–µ–µ x100, —á–µ–º Shap, Lime –Ω–∞ Mnist –∏ ImageNet
    - –∫–∞—á–µ—Å—Ç–≤–æ —Å–∏–ª—å–Ω–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ explanation_model

- **Bias in random forest variable importance measures, 2017** [[paper]](https://link.springer.com/content/pdf/10.1186/1471-2105-8-25.pdf)
  - –∏–º–ø–ª–µ–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –º–µ—Ç–æ–¥ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –¥–µ—Ä–µ–≤–∞ (ctree), –≥–¥–µ –≤—ã–±–æ—Ä –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è –ø—É—Ç–µ–º –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ –∑–Ω–∞—á–µ–Ω–∏—è p –∫—Ä–∏—Ç–µ—Ä–∏—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å–ª–æ–≤–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞, —Å—Ä–∞–≤–Ω–∏–º–æ–≥–æ, –Ω–∞–ø—Ä–∏–º–µ—Ä, —Å —Ç–µ—Å—Ç–æ–º œá2 —Å–æ —Å—Ç–µ–ø–µ–Ω—å—é —Å–≤–æ–±–æ–¥—ã, —Ä–∞–≤–Ω–æ–π —á–∏—Å–ª—É –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø—Ä–∏–∑–Ω–∞–∫–∞
  - –ª—É—á—à–µ —Å–µ–±—è –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á–µ–º rf –≤ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö (—Å/–±–µ–∑ –±—É—Ç—Å—Ç—Ä—ç–ø–æ–º, —Å–ø–æ—Å–æ–± —Å—ç–º–ø–ª–∏–Ω–≥–∞)
  - bias –≤ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ rf –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –∏–∑-–∑–∞ —Ç–æ–≥–æ, —á—Ç–æ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Ä–∞—Å–ø–æ–ª–∞–≥–∞—é—Ç—Å—è –±–ª–∏–∂–µ –∫ –∫–æ—Ä–Ω—é –¥–µ—Ä–µ–≤–∞
  - —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –¥–≤–µ –æ—Ü–µ–Ω–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞
    - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–∑–ª–æ–≤, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∏–∑–Ω–∞–∫ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –≤—ã–±–æ—Ä–∫–∏ (selection frequency)
    - permutation importance
    - Gini importance (–±–æ–ª—å—à–æ–π bias)  
  *–ò—Ç–æ–≥–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤*:
    - —Å—ç–º–ø–ª–∏–Ω–≥ —Å –≤–æ–∑–≤—Ä–∞—Ç–æ–º —Å–∏–ª—å–Ω–æ —Å–º–µ—â–∞–µ—Ç selection frequency –≤ —Å—Ç–æ—Ä–æ–Ω—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –±–æ–ª—å—à–∏–º —á–∏—Å–ª–æ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    - permutation importance –±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤

- **Grouped variable importance with random forests and application to multiple functional data analysis, 2015** [[paper]](https://arxiv.org/pdf/1411.4170.pdf)
  - —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –æ—Ü–µ–Ω–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –≥—Ä—É–ø–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–π –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Å—Ç–æ—Ä–æ–Ω—ã
  - —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è —Å—Ç–æ—Ä–æ–Ω–∞
    - (–ø—Ä–∏–∑–Ω–∞–∫–∏, —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è) - —Å–ª—É—á–∞–π–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
    - –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞ - —Ä–∞–∑–Ω–∏—Ü–∞ –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–≥–æ —Ä–∏—Å–∫–∞ —Å –∑–∞–º–µ–Ω–æ–π/–±–µ–∑ –∑–∞–º–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∞ –Ω–∞ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫, –Ω–æ –Ω–µ –∑–∞–≤–∏—Å—è—â–∏–π –æ—Ç –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    - –≤ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö –≤–∞–∂–Ω–æ—Å—Ç—å –≥—Ä—É–ø–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–∞ –¥–∏—Å–ø–µ—Ä—Å–∏–∏ —Ñ—É–Ω–∫—Ü–∏–∏ (=–º–æ–¥–µ–ª—å) –æ—Ç —ç—Ç–æ–π –≥—Ä—É–ø–ø—ã
  - –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Å—Ç–æ—Ä–æ–Ω–∞
    - –≤–∞–∂–Ω–æ—Å—Ç—å –≥—Ä—É–ø–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ - oob + —Å–ª—É—á–∞–π–Ω–∞—è –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å—Ç—Ä–æ–∫ –¥–ª—è —Å—Ç–æ–ª–±—Ü–æ–≤ –∏–∑ –≥—Ä—É–ø–ø—ã
    - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è RFE
  - —Å –ø–æ–º–æ—â—å—é –≤–µ–π–≤–ª–µ—Ç –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ä–∞–∑–±–∏–µ–Ω–∏—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –Ω–∞ –≥—Ä—É–ø–ø—ã
  - –¥–ª—è –æ—Ç–±–æ—Ä–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –≥—Ä—É–ø–ø –Ω–µ –ø—Ä–∏–º–µ–Ω–∏–º –∞–ª–≥–æ—Ä–∏—Ç–º RFE, —Ç.–∫. —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –æ–±—â–∞—è —Å–æ—Å—Ç–∞–≤–ª—è—é—â–∞—è, –≤–Ω–æ—Å—è—â–∞—è –±–æ–ª—å—à–æ–π –≤–∫–ª–∞–¥ -> –¥–µ–ª–∏–º –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä –Ω–∞ —Å–µ—Ç–∫—É –∏ –≤—ã—á–∏—Å–ª—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ç–æ—á–∫–∞—Ö  
  *–ò—Ç–æ–≥–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤*:  
    - –æ—Ü–µ–Ω–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å–æ–≥–ª–∞—Å–æ–≤—ã–≤–∞–µ—Ç—Å—è –∫–∞–∫ —Å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏, —Ç–∞–∫ –∏ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏

- **Correlation and variable importance in random forests, 2017** [[paper]](https://arxiv.org/pdf/1310.5726.pdf)
  - –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –≤—ã—à–µ–æ–ø–∏—Å–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã 
  - —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ purely rf –¥–ª—è –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å—Ö–æ–¥–∏—Ç—Å—è —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –∫ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–π –ø—Ä–∏ —Å—Ç—Ä–µ–º–ª–µ–Ω–∏–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π —Ä–∞–∑–±–∏–µ–Ω–∏—è —É–∑–ª–∞ –¥–µ—Ä–µ–≤–∞ –∏ –º–æ—â–Ω–æ—Å—Ç–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ —Ç–∞–∫, —á—Ç–æ–±—ã –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –ø–µ—Ä–≤–æ–≥–æ –∫–æ –≤—Ç–æ—Ä–æ–º—É —Å—Ç—Ä–µ–º–∏–ª–æ—Å—å –∫ 0 
  - –¥–∞–∂–µ —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –º–æ–≥—É—Ç –ø–æ–ª—É—á–∏—Ç—å –º–∞–ª—É—é –≤–∞–∂–Ω–æ—Å—Ç—å –∏–∑-–∑–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É —Å–æ–±–æ–π  
  *–ò—Ç–æ–≥–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤*:  
    - NRFE –∏ RFE –≤ —Ü–µ–ª–æ–º –∏–º–µ—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
  
- **All Models are Wrong, but Many are Useful, 2018** [[paper]](https://arxiv.org/pdf/1801.01489.pdf)
  - Raw:
    - Our  motivation  is  that  Rashomon  sets  (defined  formally  below)  summarize  therange of effective prediction strategies that an analyst might choose
    - (MR). This measure is based on permutationimportance measures for Random Forests and can be  expanded  to  describe  conditional  importance
    - beyond describing variable importance, these tools can describe the range of riskpredictions that well-fitting models assign to a particular covariate profile, or the variance ofpredictions made by well-fitting models
    - To this end, we apply a broad class of flexible, kernel-basedprediction models to predict COMPAS score
    - Equipped with MCR, wecan relax the common assumption of being able to correctly specify the unknown model ofinterest (here, COMPAS) up to a parametric form
    - //We assume that observations ofZareiid, thatn‚â•2, and thatsolutions to arg min and arg max operations exist whenever optimizing over sets mentionedin this paper (for example, in Theorem 4, below)//
    - R()  :={f‚ààF:EL(f,Z)‚â§EL(fref,Z) +}
    - efwfew wefw ![alt text](./equation.svg "Title") efwefewf wefwefew
    - feewfwefw weeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee
    - efwfew wefw ![alt text](./1.svg "Title") efwefewf wefwefe
    - feewfwefw weeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee
  - –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ–æ—Ä–º—É–ª—ã:  
  ![alt text](./1.svg "Title")![alt text](./1.svg "Title")![alt text](./1.svg "Title")![alt text](./1.svg "Title")
  
  *–ò—Ç–æ–≥–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤*:  
    - NRFE –∏ RFE –≤ —Ü–µ–ª–æ–º –∏–º–µ—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ

