

## –ö–æ–Ω—Å–ø–µ–∫—Ç—ã —Å—Ç–∞—Ç–µ–π 

- **Accurate and Robust Feature Importance Estimation under Distribution Shifts, 2020**  [[paper]](https://arxiv.org/pdf/2009.14454.pdf)

  - –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π: –æ—Å–Ω–æ–≤–Ω–∞—è —Å–µ—Ç—å –æ–±—É—á–∞–µ—Ç—Å—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π (second net), —É –∫–æ—Ç–æ—Ä–æ–π:
    - —Ü–µ–ª—å - –Ω–∞—É—á–∏—Ç—å—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å loss –æ—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ç–∏
    - input - –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª–æ—ë–≤ –æ—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ç–∏
    - loss
      - contrastive training - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–Ω–∏–µ –ø–∞—Ä —Å–∫–æ—Ä–æ–≤
      - dropout calibration - hinge loss + –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
  - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Granger –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏—á–∏–Ω–Ω–æ—Å—Ç–∏ (—Å–≤—è–∑—å –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–æ–º –∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π —Å—É—â–µ—Ç—Å–≤—É–µ—Ç, –µ—Å–ª–∏ –∫–∞—á–µ—Å—Ç–≤–æ —Ç–æ–ª—å–∫–æ —É—Ö—É–¥—à–∏—Ç—Å—è –ø—Ä–∏ –æ—Ç–±—Ä–∞—Å—ã–≤–∞–Ω–∏–∏ –¥–∞–Ω–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞)
  - –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞ - —Ä–∞–∑–Ω–∏—Ü–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–æ–π —Å–µ—Ç–∏ —Å –µ–≥–æ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –±–µ–∑  
    *–ò—Ç–æ–≥*:
    - –Ω–∞ 15-30 % –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ, —á–µ–º —É Shap
    - –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ —Ä–∞–∑–ª–∏—á–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è x_test, –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å x_train, loss second net –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ —Ä–∞—Å—Ç—ë—Ç
    - –ø–æ–¥—Ö–æ–¥ —É—Å—Ç–æ–π—á–∏–≤–µ–π –ø—Ä–∏ —Å–∏–ª—å–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö x_test Deep_Shap'–∞ –≤ 2 —Ä–∞–∑–∞

- **Feature Importance Ranking for Deep Learning, 2020** [[paper]](https://arxiv.org/pdf/2010.08973.pdf)

  - —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –¥–≤–µ —Å–µ—Ç–∏ operator net –∏ selector net, –º–∞—Å–∫–∏ –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ - –±–∏–Ω–∞—Ä–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–∞ (1 - –±–µ—Ä–µ–º –ø—Ä–∏–∑–Ω–∞–∫, 0 - –Ω–µ—Ç), –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª-–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ - –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä
  - –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø–æ–æ—á–µ—Ä–µ–¥–Ω–æ
  - operator net:
    - —Ü–µ–ª—å - –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏
    - input - x –∏ –º–∞—Å–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    - loss - —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∑–∞–¥–∞—á–µ
  - selector net:
    - —Ü–µ–ª—å - –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å loss operator net
    - input - –º–∞—Å–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    - loss - l2 —Å loss'–æ–º, –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–º –æ—Ç operator net
  - –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞ - —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ loss'–∞ selector net'–∞ –≤ —Ç–æ—á–∫–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
  - –ø—Ä–æ—Ü–µ—Å—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –æ—á–µ–Ω—å –¥–æ–ª–≥–∏–π  
    *–ò—Ç–æ–≥*:
    - –≤ —Å—Ä–µ–¥–Ω–µ–º –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    - –ª—É—á—à–µ–µ RFE, BAHSIC, mRMR, CCM –Ω–∞ 4-—ë—Ö benchmark –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö

- **Knockoffs for the mass: new feature importance statistics with false discovery guarantees, 2019** [[paper]](https://arxiv.org/pdf/1807.06214.pdf)

  - –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä—É–µ—Ç—Å—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (—Ç–æ–ª—å–∫–æ –ø—Ä–∏–∑–Ω–∞–∫–∏) –±–∞–π–µ—Å–æ–≤—Å–∫–∏–º–∏ —Å–µ—Ç—è–º–∏
  - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –≤—ã–±–æ—Ä–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–º –æ–±—Ä–∞–∑–æ–º (—á—Ç–æ–±—ã –Ω–µ –≤—ã—Ö–æ–¥–∏—Ç—å –∑–∞ –∏—Å—Ö–æ–¥–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
  - –≤–º–µ—Å—Ç–æ —Ç–æ–≥–æ, —á—Ç–æ–±—ã –ø–µ—Ä–µ–º–µ—à–∏–≤–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–∞ (–≤ permutation importance), –±–µ—Ä–µ—Ç—Å—è –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ –∏–∑ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏
  - –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞ - –ø–ª–æ—â–∞–¥—å –ø–æ–¥ –∫—Ä–∏–≤–æ–π (y - –¥–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, x - –ø–∞—Ä–∞–º–µ—Ç—Ä –≤–∑–≤–µ—à–µ–Ω–Ω–æ–π —Å—É–º–º—ã) –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, [0, 10])
  - FDR –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω–µ–ª—å–∑—è –æ—Ü–µ–Ω–∏—Ç—å, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ –º—ã —Ö–æ—Ä–æ—à–æ –º–æ–¥–µ–ª–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∫–∏

- **A Unified Approach to Interpreting Model Predictions, 2017** [[paper]](https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf)

  - —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è —Å–µ–º–µ–π—Å—Ç–≤–æ –∞–¥–¥–∏—Ç–∏–≤–Ω—ã—Ö explanation models
  - –≤ –¥–∞–Ω–Ω–æ–º –∫–ª–∞—Å—Å–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è explanation model, —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—é—â–∞—è —Å–≤–æ–π—Å—Ç–≤–∞–º:
    - local accuracy - —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π f(x) –∏ exp_model(x')
    - missingness - –ø—Ä–∏–∑–Ω–∞–∫, –Ω–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏–π –≤ x, –±—É–¥–µ—Ç –∏–º–µ—Ç—å –Ω—É–ª–µ–≤—É—é –≤–∞–∂–Ω–æ—Å—Ç—å
    - consistency - –ø—Ä–∏–∑–Ω–∞–∫ –≤–æ –≤—Å–µ–≤–æ–∑–º–æ–∂–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏—è—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∏–º–µ–µ—Ç –Ω–µ –º–µ–Ω—å—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–∞ f, —á–µ–º –Ω–∞ f' -> –µ–≥–æ –≤–∞–∂–Ω–æ—Å—Ç—å –¥–ª—è f >= –≤–∞–∂–Ω–æ—Å—Ç—å –¥–ª—è f'
  - —Å—á–∏—Ç–∞—Ç—å —Ç–∞–∫—É—é explanation model –¥–æ—Ä–æ–≥–æ
  - Linear LIME + Kernel SHAP –¥–∞—é—Ç –∏—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è SHAP values
  - –≤ —Å–ª—É—á–∞–µ f = max, –º–æ–∂–Ω–æ –∑–∞ –∫–≤–∞–¥—Ä–∞—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ SHAP –ø–æ—Å—á–∏—Ç–∞—Ç—å (–ø–æ–ª—å–∑—É–µ–º—Å—è —Å–≤–æ–π—Å—Ç–≤–∞–º–∏ max)
  - Deep SHAP - –≤–º–µ—Å—Ç–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –≤ DeepLIFT –ø–æ–¥—Å—Ç–∞–≤–ª—è–µ–º SHAP –≤–∞–∂–Ω–æ—Å—Ç—å –¥–ª—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–∞—Å—á—ë—Ç–∞—Ö  
    *–ò—Ç–æ–≥*:
    - –æ–±–æ–±—â–∏–ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–µ—Ç–æ–¥—ã
    - –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–π –∑–∞–¥–∞—á–µ SHAP –≤–∞–∂–Ω–æ—Å—Ç—å —Å–æ–≤–ø–∞–ª–∞ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π
    - –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —É–ª—É—á—à–∏–ª–∏ –≤—Ä–µ–º—è —Ä–∞—Å—á—ë—Ç–∞

- **Learning Important Features Through Propagating Activation Differences, 2017** [[paper]](https://arxiv.org/pdf/1704.02685.pdf)

  - –º–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Ä–∞–∑–Ω–∏—Ü–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ–π—Ä–æ–Ω–æ–≤ –º–µ–∂–¥—É –Ω–∞—á–∞–ª—å–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º (reference) –∏ –∫–æ–Ω–µ—á–Ω—ã–º
  - —Ä–∞–∑–¥–µ–ª—è—é—Ç—Å—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –≤–∫–ª–∞–¥—ã –≤ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
  - –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞ –ª–∏–Ω–µ–π–Ω–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ä–∞–∑–Ω–∏—Ü—ã x - x_reference
  - –≤–∞–∂–Ω–æ—Å—Ç—å - shapley value —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ä–∞–∑–±–∏–µ–Ω–∏–π 2
  - x_reference –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–¥–∞—á–∏  
    *–ò—Ç–æ–≥*:
    - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω–æ—Å—Ç–∏ x - x_reference –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è—Ç—å—Å—è –∫–æ–≥–¥–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç —Ä–∞–≤–µ–Ω –Ω—É–ª—é
    - gradients, gradients*input, guided backprop, rescale rule —Ç–µ—Ä—è—é—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤ —Ö–æ–¥–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö –≤ –æ—Ç–ª–∏—á–∏–∏ –æ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ reveal_cancel rule

- **CXPlain: Causal Explanations for Model Interpretation under Uncertainty, 2019** [[paper]](https://arxiv.org/pdf/1910.12336.pdf)

  - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Granger's definition of causality (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏, –∏—Å—Ö–æ–¥—è —Ç–æ–ª—å–∫–æ –∏–∑ –¥–∞–Ω–Ω—ã—Ö, –Ω–µ–ª—å–∑—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å)
    - –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ
    - –ø—Ä–∏–∑–Ω–∞–∫ –≤—Ä–µ–º–µ–Ω–Ω–æ –ø—Ä–µ–¥—à–µ—Å—Ç–≤—É–µ—Ç –º–µ—Ç–∫–µ (–¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –º–µ—Ç–∫—É, –Ω—É–∂–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–∏–∑–Ω–∞–∫–µ)
  - –∏—Å—Ç–∏–Ω–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞ - –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –æ—à–∏–±–æ–∫ –æ–±—ä—è—Å–Ω—è–µ–º–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ x_mask –∏ x_reference
  - –æ–±—É—á–∞–µ—Ç—Å—è explanation model (–ø–æ–¥—Ö–æ–¥—è—â–∞—è —Ä–µ—à–∞–µ–º–æ–π –∑–∞–¥–∞—á–µ)
    - —Ü–µ–ª—å - –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    - input - –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç x_train
    - loss - —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –ö—É–ª—å–±–∞–∫–∞ ‚Äî –õ–µ–π–±–ª–µ—Ä–∞ –º–µ–∂–¥—É –∏—Å—Ç–∏–Ω–Ω—ã–º –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏ –≤–∞–∂–Ω–æ—Å—Ç–µ–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
  - –¥–ª—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –æ–±—É—á–∞–µ–º –∞–Ω—Å–∞–º–±–ª—å explanation_models (–Ω–∞ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤—ã–±–æ—Ä–∫–∞—Ö), –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å - –º–µ–¥–∏–∞–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∞–Ω—Å–∞–º–±–ª—è, –∞ —Ç–æ—á–Ω–æ—Å—Ç—å - –∏–Ω—Ç–µ—Ä–∫–≤–∞–Ω—Ç–∏–ª—å–Ω—ã–π —Ä–∞–∑–º–∞—Ö  
    *–ò—Ç–æ–≥*:
    - —Ç–æ—á–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç —Å –æ—à–∏–±–∫–æ–π —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    - –ø—Ä–∏ –Ω–µ–±–æ–ª—å—à–æ–π –º–æ—â–Ω–æ—Å—Ç–∏ –∞–Ω—Å–∞–º–±–ª—è (5) —Ö–æ—Ä–æ—à–æ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —Ç–æ—á–Ω–æ—Å—Ç—å explanation_model
    - –∫–∞—á–µ—Å—Ç–≤–æ –ª—É—á—à–µ –Ω–∞ 20%, –±—ã—Å—Ç—Ä–µ–µ x100, —á–µ–º Shap, Lime –Ω–∞ Mnist –∏ ImageNet
    - –∫–∞—á–µ—Å—Ç–≤–æ —Å–∏–ª—å–Ω–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ explanation_model

- **Bias in random forest variable importance measures, 2017** [[paper]](https://link.springer.com/content/pdf/10.1186/1471-2105-8-25.pdf)

  - –∏–º–ø–ª–µ–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –º–µ—Ç–æ–¥ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –¥–µ—Ä–µ–≤–∞ (ctree), –≥–¥–µ –≤—ã–±–æ—Ä –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è –ø—É—Ç–µ–º –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ –∑–Ω–∞—á–µ–Ω–∏—è p –∫—Ä–∏—Ç–µ—Ä–∏—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å–ª–æ–≤–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞, —Å—Ä–∞–≤–Ω–∏–º–æ–≥–æ, –Ω–∞–ø—Ä–∏–º–µ—Ä, —Å —Ç–µ—Å—Ç–æ–º œá2 —Å–æ —Å—Ç–µ–ø–µ–Ω—å—é —Å–≤–æ–±–æ–¥—ã, —Ä–∞–≤–Ω–æ–π —á–∏—Å–ª—É –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø—Ä–∏–∑–Ω–∞–∫–∞
  - –ª—É—á—à–µ —Å–µ–±—è –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á–µ–º rf –≤ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö (—Å/–±–µ–∑ –±—É—Ç—Å—Ç—Ä—ç–ø–æ–º, —Å–ø–æ—Å–æ–± —Å—ç–º–ø–ª–∏–Ω–≥–∞)
  - bias –≤ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ rf –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –∏–∑-–∑–∞ —Ç–æ–≥–æ, —á—Ç–æ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Ä–∞—Å–ø–æ–ª–∞–≥–∞—é—Ç—Å—è –±–ª–∏–∂–µ –∫ –∫–æ—Ä–Ω—é –¥–µ—Ä–µ–≤–∞
  - —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –¥–≤–µ –æ—Ü–µ–Ω–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞
    - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–∑–ª–æ–≤, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∏–∑–Ω–∞–∫ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –≤—ã–±–æ—Ä–∫–∏ (selection frequency)
    - permutation importance
    - Gini importance (–±–æ–ª—å—à–æ–π bias)  
      *–ò—Ç–æ–≥–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤*:
    - —Å—ç–º–ø–ª–∏–Ω–≥ —Å –≤–æ–∑–≤—Ä–∞—Ç–æ–º —Å–∏–ª—å–Ω–æ —Å–º–µ—â–∞–µ—Ç selection frequency –≤ —Å—Ç–æ—Ä–æ–Ω—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –±–æ–ª—å—à–∏–º —á–∏—Å–ª–æ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    - permutation importance –±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤

- **Grouped variable importance with random forests and application to multiple functional data analysis, 2015** [[paper]](https://arxiv.org/pdf/1411.4170.pdf)

  - —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –æ—Ü–µ–Ω–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –≥—Ä—É–ø–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–π –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Å—Ç–æ—Ä–æ–Ω—ã
  - —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è —Å—Ç–æ—Ä–æ–Ω–∞
    - (–ø—Ä–∏–∑–Ω–∞–∫–∏, —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è) - —Å–ª—É—á–∞–π–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
    - –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞ - —Ä–∞–∑–Ω–∏—Ü–∞ –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–≥–æ —Ä–∏—Å–∫–∞ —Å –∑–∞–º–µ–Ω–æ–π/–±–µ–∑ –∑–∞–º–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∞ –Ω–∞ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫, –Ω–æ –Ω–µ –∑–∞–≤–∏—Å—è—â–∏–π –æ—Ç –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    - –≤ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö –≤–∞–∂–Ω–æ—Å—Ç—å –≥—Ä—É–ø–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–∞ –¥–∏—Å–ø–µ—Ä—Å–∏–∏ —Ñ—É–Ω–∫—Ü–∏–∏ (=–º–æ–¥–µ–ª—å) –æ—Ç —ç—Ç–æ–π –≥—Ä—É–ø–ø—ã
  - –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Å—Ç–æ—Ä–æ–Ω–∞
    - –≤–∞–∂–Ω–æ—Å—Ç—å –≥—Ä—É–ø–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ - oob + —Å–ª—É—á–∞–π–Ω–∞—è –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å—Ç—Ä–æ–∫ –¥–ª—è —Å—Ç–æ–ª–±—Ü–æ–≤ –∏–∑ –≥—Ä—É–ø–ø—ã
    - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è RFE
  - —Å –ø–æ–º–æ—â—å—é –≤–µ–π–≤–ª–µ—Ç –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ä–∞–∑–±–∏–µ–Ω–∏—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –Ω–∞ –≥—Ä—É–ø–ø—ã
  - –¥–ª—è –æ—Ç–±–æ—Ä–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –≥—Ä—É–ø–ø –Ω–µ –ø—Ä–∏–º–µ–Ω–∏–º –∞–ª–≥–æ—Ä–∏—Ç–º RFE, —Ç.–∫. —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –æ–±—â–∞—è —Å–æ—Å—Ç–∞–≤–ª—è—é—â–∞—è, –≤–Ω–æ—Å—è—â–∞—è –±–æ–ª—å—à–æ–π –≤–∫–ª–∞–¥ -> –¥–µ–ª–∏–º –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä –Ω–∞ —Å–µ—Ç–∫—É –∏ –≤—ã—á–∏—Å–ª—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ç–æ—á–∫–∞—Ö  
    *–ò—Ç–æ–≥–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤*:  
    - –æ—Ü–µ–Ω–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å–æ–≥–ª–∞—Å–æ–≤—ã–≤–∞–µ—Ç—Å—è –∫–∞–∫ —Å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏, —Ç–∞–∫ –∏ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏

- **Correlation and variable importance in random forests, 2017** [[paper]](https://arxiv.org/pdf/1310.5726.pdf)

  - –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –≤—ã—à–µ–æ–ø–∏—Å–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã 
  - —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ purely rf –¥–ª—è –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å—Ö–æ–¥–∏—Ç—Å—è —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –∫ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–π –ø—Ä–∏ —Å—Ç—Ä–µ–º–ª–µ–Ω–∏–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π —Ä–∞–∑–±–∏–µ–Ω–∏—è —É–∑–ª–∞ –¥–µ—Ä–µ–≤–∞ –∏ –º–æ—â–Ω–æ—Å—Ç–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ —Ç–∞–∫, —á—Ç–æ–±—ã –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –ø–µ—Ä–≤–æ–≥–æ –∫–æ –≤—Ç–æ—Ä–æ–º—É —Å—Ç—Ä–µ–º–∏–ª–æ—Å—å –∫ 0 
  - –¥–∞–∂–µ —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –º–æ–≥—É—Ç –ø–æ–ª—É—á–∏—Ç—å –º–∞–ª—É—é –≤–∞–∂–Ω–æ—Å—Ç—å –∏–∑-–∑–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É —Å–æ–±–æ–π  
    *–ò—Ç–æ–≥–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤*:  
    - NRFE –∏ RFE –≤ —Ü–µ–ª–æ–º –∏–º–µ—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ

- **All Models are Wrong, but Many are Useful, 2018** [[paper]](https://arxiv.org/pdf/1801.01489.pdf)
  –ò–¥–µ—è - –±—É–¥–µ–º –∏—Å–∫–∞—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –≥—Ä—É–ø–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ X_1 –Ω–µ –¥–ª—è –æ–¥–Ω–æ–π —Ö–æ—Ä–æ—à–µ–π –º–æ–¥–µ–ª–∏ (reference model), –∞ –¥–ª—è –∫–ª–∞—Å—Å–∞ –º–æ–¥–µ–ª–µ–π
  –î–∞—Ç–∞—Å–µ—Ç - iid
  –í–≤–µ–¥—ë–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π:
  a population Œµ-Rashomon  set:  <img src="image-20210216213004399.png" width=47% height=50%>

  model relience: <img src="image-20210216214252202.png" width=47% height=50%>

  a population-level model class reliance (MCR) range: ![img_5.png](img_5.png)
  the set of functionsGras anr-margin-expectation-coverif for anyf‚àà Fand anydistributionD, there existsg‚ààGrsuch that![img_12.png](img_12.png)
  *the covering number N(F,r)* to be the size of the smallest-margin-expectation-cover for F

  - If MCR+() is low, then no well-performing model in F places high importance on feature

  –í–æ–∑–º–æ–∂–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –≤–∞—Ä–∏–∞—Ü–∏–∏ empirical MR: ![img_6.png](img_6.png) /divide 
  ![img_7.png](img_7.png) ![img_8.png](img_8.png) ![img_9.png](img_9.png)

  - The estimators  ÀÜeorig(f),  ÀÜeswitch(f) and  ÀÜedivide(f) all belong to the well-studied class ofU-statistics.  Thus, under fairly minor conditions, these estimators are *unbiased, asymptot-ically normal, and have finite-sample probabilistic bounds*
  - Errors are calculated on the train dataset

  We introduce three bounded loss assumptions: ![img_10.png](img_10.png)

  - some constants can be derived from others

  ![img_11.png](img_11.png)

  - As n increases, out approachesandQoutapproaches zero
    ![img_13.png](img_13.png)
  - the largest possible estimation error forMR(f) across all models inFis bounded byq(Œ¥,r,n), which can be made arbitrarily smallby increasingnand decreasingr. 
  - The  existence  of  this  uniform  bound  implies  that  it  is  feasible  to  train  a  model  and  to evaluate its importance using the *same data*
    ![img_14.png](img_14.png)
    -

  Calculating Empirical Estimates of Model Class Reliance (MCR_-)

  - ComputingÃÇMCR+() however will require that we are able to minimize arbitrary linearcombinations of ÀÜeorig(f) and ÀÜeswitch(f). 
  - we present bound functionsb‚àíandb+satisfyingb‚àí(abs)‚â§ÃÇMR(f)‚â§b+(abs) simultaneously for all{f,abs:  ÀÜeorig(f)‚â§abs,f‚ààF,abs>0}(Figures2 & 8 show examples of these bounds).  The binary search procedures we propose can beused to tighten these boundaries at a particular valueabsof interest.
  - Almost all of the results shown in this section, and those in Section 6.2, also hold if wereplace ÀÜeswitchwith ÀÜedividethroughout (see Eq 3.5), including in the definition ofÃÇMRandÀÜh‚àí,Œ≥(f). 
  - ![img_15.png](img_15.png)
  - ![img_16.png](img_16.png)
  - It remains to determine which value ofŒ≥should be used in Eq 6.1.  The following lemmaimplies that this value can be determined by a binary search, given a particular value ofinterest forabs.
  - ![img_17.png](img_17.png)
    - he tightest lower bound from Eq 6.1 occurs whenŒ≥is as low aspossible while still satisfying Condition 8.
  - ![img_18.png](img_18.png) ![img_19.png](img_19.png)
    - –ï—Å–ª–∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è, —Ç–æ minimizingÀÜh‚àí,Œ≥forŒ≥‚â•0 is equivalent to minimizing areweighted empirical loss over an expanded sample of sizen2
      ![img_20.png](img_20.png)
    - —Å rf —É—Å–ª–æ–≤–∏—è –Ω–µ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è 

  Calculating Empirical Estimates of Model Class Reliance (MCR_+)

  - ![img_21.png](img_21.png)

  - ![img_22.png](img_22.png) ![img_23.png](img_23.png)
    Convex Models

  - –∏–¥–µ—è: –ø—É—Å—Ç—å —Ñ—É–Ω–∫—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑—É—é—Ç—Å—è –Ω–µ–∫–æ—Ç–æ—Ä—ã–º –≤–µ–∫—Ç–æ—Ä–æ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö, —Ä–∞–∑–æ–±—å—ë–º —ç—Ç–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –Ω–∞ —Å–∏–º–ø–ª–µ–∫—Å—ã, –Ω–∞ –Ω–∏—Ö h —Å–æ–≤–ø–∞–¥–∞–µ—Ç –≤ –≤–µ—Ä—à–∏–Ω–∞—Ö —Å –Ω–µ–∫–æ—Ç–æ—Ä–æ–π –≥–∏–ø–µ—Ä–ø–ª–æ—Å–∫–æ—Å—Ç—å—é, –∑–∞–º–µ–Ω—è–µ–º h –µ—ë, –ø–æ–ª—É—á–∞–µ–º –Ω–∏–∂–Ω–∏—é –æ—Ü–µ–Ω–∫—É, —Ç–∞–∫ –¥–ª—è –≤—Å–µ—Ö –ø–æ–¥–≤—ã–±–æ—Ä–æ–∫ –∏–∑ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∏ –∏–Ω–¥—É–∫—Ç–∏–≤–Ω–æ –ø–æ–≤—Ç–æ—Ä—è–µ–º –ø—Ä–æ—Ü–µ–¥—É—Ä—É
    MR & MCR for Linear Models, Additive Models

  - ![img_24.png](img_24.png)

  - ![img_25.png](img_25.png)

    - —Å–ª–æ–∂–Ω–æ—Å—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ä–∞—Å—Ç—ë—Ç –ª–∏–Ω–µ–π–Ω–æ

  - ![img_26.png](img_26.png)

  - ![img_27.png](img_27.png)

  - The resulting optimization problem is a (possibly non-convex) quadratic program withone  quadratic  constraint 

  - ![img_28.png](img_28.png)
    Regression Models in a Reproducing Kernel Hilbert Space

  - ![img_29.png](img_29.png)
    Calculating MCR

  - ![img_30.png](img_30.png)
    Upper Bounding the Loss
    ![img_31.png](img_31.png)
    Model Reliance and Causal Effects

  - ![img_33.png](img_33.png) ![img_34.png](img_34.png) ![img_32.png](img_32.png)
    Conditional Importance:  Adjusting for Dependence BetweenX1andX2

  - ![img_35.png](img_35.png) ![img_36.png](img_36.png)

  - This means thatCMR will not be influenced by impossible combinations ofx1andx2, while MR may beinfluenced by them
    Estimation of CMR by Weighting, Matching, or Imputation

  - ![img_37.png](img_37.png) ![img_38.png](img_38.png)

  - ![img_40.png](img_40.png) ![img_39.png](img_39.png)
    However, when the covariate space is continuous or high dimensional, we typically cannotestimate  CMR  nonparametrically.
    when the covariate space is continuous or high dimensional
    ![img_41.png](img_41.png)
    Simulations of Bootstrap Confidence Intervals  

  - –∏–¥–µ—è: 
    1 –ø–æ–¥—Ö–æ–¥
    –≤–æ–∑—å–º—ë–º –æ—Ä–∏–≥. –¥–∞—Ç–∞—Å–µ—Ç (20k –∑–∞–ø–∏—Å–µ–π), –ø–æ—Å—á–∏—Ç–∞–µ–º –Ω–∞ –Ω–µ–º MCR, —Ä–∞–∑–¥–µ–ª–∏–º –≤–µ—Å—å –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ 2 —á–∞—Å—Ç–∏
    training subset and analysis subset

    - –Ω–∞ training subset: –æ–±—É—á–∞–µ–º reference model
    - –Ω–∞ analysis subset: —Å—ç–º–ø–ª–∏—Ä—É–µ–º –≤—ã–±–æ—Ä–∫—É (500 times) –∏ —Å—á–∏—Ç–∞–µ–º emp_MCR_- –∏ emp_MCR_+, –∞ –ø–æ—Å–ª–µ CI

    2 –ø–æ–¥—Ö–æ–¥ (–ø—Ä–æ—â–µ)
    c—ç–º–ø–ª–∏—Ä—É–µ–º –≤—ã–±–æ—Ä–∫—É —Å –æ—Ä–∏–≥. –¥–∞—Ç–∞—Å–µ—Ç–∞ (500 times), –¥–µ–ª–∏–º –µ–≥–æ –Ω–∞ 2 —á–∞—Å—Ç–∏

    - –Ω–∞ 1–æ–π —á–∞—Å—Ç–∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    - –Ω–∞ 2–æ–π –æ—Ü–µ–Ω–∏–≤–∞–µ–º –µ—ë emp_MR
      –ø–æ–ª—É—á–∞–µ–º CI
      –ò—Ç–æ–≥: 1 –ø–æ–¥—Ö–æ–¥ more robust to the misspecification of the models used to approximate Y and the model of Y itself

  COMPAS score
  The bootstrap 95% CI for MCR on ‚Äúinadmissiblevariables‚Äù is [1.00, 1.73]
  For ‚Äúadmissible variables‚Äù the MCR range with a 95% bootstrapCI is equal to [1.62, 3.96]








  *–ò—Ç–æ–≥–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤*:  
Assumption\ 1\ (Bounded\ individual\ loss)\ For\ a\ given\ model\ f\ \in\ \mathcal{F},\ assume\ that\ 0 \leq L\left(f,\left(y, x_{1}, x_{2}\right)\right) \leq B_{\text {ind }} for any \left(y, x_{1}, x_{2}\right) \in\left(\mathcal{Y} \times \mathcal{X}_{1} \times \mathcal{X}_{2}\right)
    -

